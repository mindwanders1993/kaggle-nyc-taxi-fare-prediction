{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e01af35",
   "metadata": {},
   "source": [
    "# New York Taxi Fare Prediction\n",
    "\n",
    "Can you predict a rider's taxi fare?\n",
    "\n",
    "Kaggle: https://www.kaggle.com/competitions/new-york-city-taxi-fare-prediction/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47171f58",
   "metadata": {},
   "source": [
    "Project outline:\n",
    "1. Download/ Load the dataset\n",
    "2. Explore & analyze the dataset\n",
    "3. Prepare the dataset for ML training\n",
    "4. Train hardcoded & baseline models\n",
    "5. Make predictions & submit to Kaggle\n",
    "6. Perform feature engineering\n",
    "7. Train & evaluate different models\n",
    "8. Tune hyperparameters for the best models\n",
    "9. Train on a GPU with the entire dataset\n",
    "10. Document & publish the project online"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f9e370",
   "metadata": {},
   "source": [
    "## 1. Download/ Load the dataset\n",
    "\n",
    "Steps:\n",
    "* Install required libraries\n",
    "* Download data from Kaggle\n",
    "* View dataset files\n",
    "* Load training set in dataframe\n",
    "* Load test set in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import time package\n",
    "import time\n",
    "\n",
    "# Import dask libraries\n",
    "import dask\n",
    "from dask import dask_cudf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5571e046",
   "metadata": {},
   "source": [
    "### View Dataset Files\n",
    "\n",
    "#### Use Shell commands to view the large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a002e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path of the directory\n",
    "data_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a35b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files with size from directory\n",
    "!ls -lh {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d48efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training set\n",
    "!head {data_dir}/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19123050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test set\n",
    "!head {data_dir}/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e80ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sample submission file\n",
    "!head {data_dir}/sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad7f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of lines in training set\n",
    "!wc -l {data_dir}/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36830e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of lines in test set\n",
    "!wc -l {data_dir}/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of lines in submission file\n",
    "!wc -l {data_dir}/sample_submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ad9b0",
   "metadata": {},
   "source": [
    "Observation:\n",
    "* It is a supervised learning regression problem\n",
    "* Size of training data is 5.5 GB\n",
    "* Training data has 5.5 million rows\n",
    "* Test data has < 10000 rows\n",
    "* Training set has 8 columns:\n",
    "    * `key` (unique identifier)\n",
    "    * `fare_amount` (target column)\n",
    "    * `pickup_datetime`\n",
    "    * `pickup_longitude`\n",
    "    * `pickup_latitude`\n",
    "    * `dropoff_longitude`\n",
    "    * `dropoff_latitude`\n",
    "    * `passenger_count`\n",
    "* Test set does not include target colum `fare_amount`\n",
    "* Submission file contains `key` and `fare_amount` column for each test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f1943",
   "metadata": {},
   "source": [
    "### Loading Training Set\n",
    "\n",
    "> Tip: When working with large datasets, always start with a sample to experiment & iterate faster\n",
    "\n",
    "Loading entire datasets into Pandas is very slow, use the following optimizations:\n",
    "* Ignore the `key` column\n",
    "* Parse `pickup_datetime` while loading the data\n",
    "* Specify data types for other columns\n",
    "    * `float32` for geo coordinates\n",
    "    * `float32` for fare \n",
    "    * `uint8` for passenger count\n",
    "* Work with 1% sample data (~500k rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040be2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1fa230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set sample fraction of dataset\n",
    "sample_fraction = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "# defined selected columns\n",
    "selected_cols = ['fare_amount', 'pickup_datetime', 'pickup_longitude',\n",
    "                 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\n",
    "\n",
    "# defined data types\n",
    "dtypes = {\n",
    "    'fare_amount': 'float32',\n",
    "    'pickup_longitude': 'float32',\n",
    "    'pickup_latitude': 'float32',\n",
    "    'dropoff_longitude': 'float32',\n",
    "    'dropoff_latitude': 'float32',\n",
    "    'passenger_count': 'uint8'\n",
    "}\n",
    "\n",
    "# function for selecting random rows of data\n",
    "def skip_row(row_idx):\n",
    "    if row_idx == 0:\n",
    "        return False\n",
    "    return random.random() > sample_fraction\n",
    "\n",
    "# Set random seed\n",
    "random.seed(42)\n",
    "\n",
    "# Load data to a pandas dataframe\n",
    "df = pd.read_csv(data_dir+'/train.csv', usecols=selected_cols, \n",
    "                       parse_dates=['pickup_datetime'], dtype=dtypes, \n",
    "                       skiprows=skip_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20abba03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show train dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e4c4ea",
   "metadata": {},
   "source": [
    "***fix seeds for random number generators, so that we can get same results everytime we run the notebook.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdbc43b",
   "metadata": {},
   "source": [
    "### Load Test Set\n",
    "\n",
    "Update data types and parse datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "test_df = pd.read_csv(data_dir+'/test.csv', dtype=dtypes, parse_dates=['pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0982a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show test dataset\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dbc28b",
   "metadata": {},
   "source": [
    "## 2. Explore & Analyze Dataset\n",
    "\n",
    "* Basic info about training set\n",
    "* Basic info about test set\n",
    "* Exploratory data analysis & visualization\n",
    "* Ask and answer questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfebd28",
   "metadata": {},
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b8e47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show train set information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a4313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show summary statistics of train set\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe3ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show datetime ranges\n",
    "df['pickup_datetime'].min(), df['pickup_datetime'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d3c76",
   "metadata": {},
   "source": [
    "Observations about training data:\n",
    "* 550K+ rows\n",
    "* No missing data in sample data\n",
    "* `fare_amount` ranges from `$-52.0` to `$499.0`\n",
    "* `passenger_count` ranges from 0 to 208\n",
    "* There are errors in longitude and latitude values (outliers are there)\n",
    "* `pickup_datetime` ranges from 1st Jan 2009 to 30th June 2015\n",
    "* Sample data takes memory up to ~16MB in RAM\n",
    "\n",
    "***We may need to deal with outliers and data entry error before training ML model***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b6f110",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show test set information\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show summary statistics of test set\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8325e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show datetime ranges\n",
    "test_df['pickup_datetime'].min(), test_df['pickup_datetime'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9736d178",
   "metadata": {},
   "source": [
    "Observations about test set:\n",
    "* 9914 rows of data\n",
    "* No missing values\n",
    "* No obvious data entry errors\n",
    "* `passenger_count` ranges between 1 to 6 passengers (we can limit trianing data to this range)\n",
    "* Latitude lies between 40 and 42\n",
    "* Longitude lies between -74 and -73\n",
    "* `pickup_datetime` ranges from 1st Jan 2009 to 30th June 2015 (same as training set)\n",
    "\n",
    "***We can use this ranges of the test set to drop outliers/ invalid data from training set.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5794ea",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis and Visualization\n",
    "\n",
    "**Tasks**: Create graphs (histograms, line charts, bar charts, scatter plots, box plots, geo maps etc.) to study the distribution of values in each column, and the relationship of each input column to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d2531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80da4209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4ed34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48121af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf26880c",
   "metadata": {},
   "source": [
    "### Ask & Answer Questions\n",
    "\n",
    "Questions:\n",
    "1. What is the busiest day of the week?\n",
    "2. What is the busiest time of the day?\n",
    "3. In which month are fares the highest?\n",
    "4. Which pickup locations have the highest fares?\n",
    "5. Which drop locations have the highest fares?\n",
    "6. What is the average ride distance?\n",
    "\n",
    "> Understanding the data using EDA will give ideas for feature engineering.\n",
    "\n",
    "> Iterative approach building ML models: do some EDA, do some feature engineering, train a model, repeat to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c7a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec12b4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "420a994d",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset for ML Training\n",
    "\n",
    "* Split Training & Validation Set\n",
    "* Fill/ Remove Missing Values\n",
    "* Extract Inputs & Outputs\n",
    "    * Training \n",
    "    * Validation\n",
    "    * Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eecd13",
   "metadata": {},
   "source": [
    "### Split Training & Validation Set\n",
    "\n",
    "Set aside 20% of training data as validation set, to evaluate the train models. Pick random 20% fraction as test set and training set have same date ranges .\n",
    "\n",
    "> TIP: Validation set should be similar to test set or real world data as close as possible, i.e. evaluation score of a model on validation & test sets should be very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f191d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package from sklearn for splitting data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and validation set\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb439c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check row count\n",
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6f2a7",
   "metadata": {},
   "source": [
    "### Fill/ Remove Missing Values\n",
    "\n",
    "There were no missing values in sample data, but if there were, we will drop the rows with missing values instead of trying to fill them (since we have lot of training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55efe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values\n",
    "train_df = train_df.dropna()\n",
    "val_df = val_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd56c12",
   "metadata": {},
   "source": [
    "### Extract Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440026db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined input columns\n",
    "input_cols = ['pickup_longitude', 'pickup_latitude',\n",
    "              'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\n",
    "\n",
    "# defined target column\n",
    "target_col = 'fare_amount'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e61e9a",
   "metadata": {},
   "source": [
    "### Training Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training inputs data\n",
    "train_inputs = train_df[input_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training target data\n",
    "train_targets = train_df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fef273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show train inputs data\n",
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show train target data\n",
    "train_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab51f46",
   "metadata": {},
   "source": [
    "### Validation Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323deea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation inputs data\n",
    "val_inputs = val_df[input_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f0c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation target data\n",
    "val_targets = val_df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb978a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show validation inputs data\n",
    "val_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show validation target data\n",
    "val_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27188cf3",
   "metadata": {},
   "source": [
    "### Test Inputs\n",
    "\n",
    "It will not have target data, which we have to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7841924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test inputs data\n",
    "test_inputs = test_df[input_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503bd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show test target data\n",
    "test_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd51cb",
   "metadata": {},
   "source": [
    "## 4. Train Hardcoded & Baseline models\n",
    "\n",
    "> TIP: Always create a simple hardcoded or baseline model to establish the minimum score any proper ML model should beat.\n",
    "\n",
    "* Hardcoded model: always predict average fare\n",
    "* Baseline model: Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd28494",
   "metadata": {},
   "source": [
    "### Train & Evaluate Hardcoded Model\n",
    "\n",
    "Create a simple model that always predict the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20586a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy package\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for training and predicting average\n",
    "class MeanRegressor:\n",
    "    def fit(self, inputs, targets):\n",
    "        self.mean = targets.mean()\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        return np.full(inputs.shape[0], self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a596fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate the model\n",
    "mean_model = MeanRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d3cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fit function from class\n",
    "mean_model.fit(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56268766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean from the model\n",
    "mean_model.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08656d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the mean from model using train set\n",
    "train_preds = mean_model.predict(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d6665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show prediction results on train set\n",
    "train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d966a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show actual targets from train set\n",
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926fd60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the mean from model using validation set\n",
    "val_preds = mean_model.predict(val_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b8952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show prediction results on validation set\n",
    "val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ecea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show actual targets from validation set\n",
    "val_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f48d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics packages from sklearn\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate mean squared error\n",
    "def rmse(targets, preds):\n",
    "    return mean_squared_error(targets, preds, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1705ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate train loss (mean squared error)\n",
    "train_rmse = rmse(train_targets, train_preds)\n",
    "train_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate validation loss (mean squared error)\n",
    "val_rmse = rmse(val_targets, val_preds)\n",
    "val_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2b9a8",
   "metadata": {},
   "source": [
    "Hard-coded model is off by `$9.899` on average, which is pretty bad considering average fare is `$11.35`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bbc266",
   "metadata": {},
   "source": [
    "### Train & Evaluate Baseline Model\n",
    "\n",
    "Train a linear regression model as our baseline model, which tries to express the target as weighted sum of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import linear regression package from sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73870daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the linear regression\n",
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82230ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to training data\n",
    "linear_model.fit(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ad684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on training data\n",
    "train_preds = linear_model.predict(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show prediction on training data\n",
    "train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15dbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate training loss (root mean squared error)\n",
    "rmse(train_targets, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on validation data\n",
    "val_preds = linear_model.predict(val_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show prediction on validation data\n",
    "val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8845c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate validation loss (root mean squared error)\n",
    "rmse(val_targets, val_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa595d",
   "metadata": {},
   "source": [
    "Linear regression model is off by `$9.899`, which isn't much better than simply predicting the average.\n",
    "\n",
    "This is mainly because, training data(geocoordinates) is not in a format that is useful for the model, we are also not using most important columm `pickup_datetime`\n",
    "\n",
    "Now, proper model should beat the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da53f99",
   "metadata": {},
   "source": [
    "## 5. Make Predictions & Submit to Kaggle\n",
    "\n",
    "* Make predictions for test set\n",
    "* Generate submissions CSV\n",
    "* Submit to Kaggle\n",
    "* Record in experiment tracking sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show test inputs\n",
    "test_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be596de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "test_preds = linear_model.predict(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb50655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show prediction results on test data\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfc8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample submission data\n",
    "submission_df = pd.read_csv(data_dir+'/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eff1f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show sample submission data\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417a1f2",
   "metadata": {},
   "source": [
    "Test data and sample submission data has same number of rows and key columns are same in both dataset, so we just need to update the `fare_amount` column with new prediction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a49bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now replace the fare_amount with test predictions\n",
    "submission_df['fare_amount'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b98ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show new sample submission data\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf465f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save submission data to CSV\n",
    "submission_df.to_csv('linear_model_submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6999af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create submission file\n",
    "def generate_submission(test_preds, fname):\n",
    "    sub_df = pd.read_csv(data_dir+'/sample_submission.csv')\n",
    "    sub_df['fare_amount'] = test_preds\n",
    "    sub_df.to_csv(fname, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4439665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file for linear_model\n",
    "generate_submission(test_preds, 'linreg_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a1ca8",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "\n",
    "> TIP: Take an iterative approach to feature engineering. Add some features, train a model, evaluate it, keep the features if they help, otherwise drop them, then repeat.\n",
    "\n",
    "* Extract parts of date\n",
    "* Remove outliers & invalid data\n",
    "* Add distance between pickup & drop\n",
    "* Add distance from landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5df55c",
   "metadata": {},
   "source": [
    "### Extract Parts of Date\n",
    "* Year\n",
    "* Month\n",
    "* Day\n",
    "* Weekday\n",
    "* Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54dc595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function extract parts from datetime\n",
    "def add_dateparts(df, col):\n",
    "    df[col + '_year'] = df[col].dt.year\n",
    "    df[col + '_month'] = df[col].dt.month\n",
    "    df[col + '_day'] = df[col].dt.day\n",
    "    df[col + '_weekday'] = df[col].dt.weekday\n",
    "    df[col + '_hour'] = df[col].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add date parts in train set\n",
    "add_dateparts(train_df, 'pickup_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8075096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add date parts in validation set\n",
    "add_dateparts(val_df, 'pickup_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e0ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add date parts in test set\n",
    "add_dateparts(test_df, 'pickup_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73367fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show train set\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e924e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show validation set\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show test set\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc22e25",
   "metadata": {},
   "source": [
    "### Add Distance Between Pickup and Drop Location\n",
    "\n",
    "We can use the haversine distance: \n",
    "- https://en.wikipedia.org/wiki/Haversine_formula\n",
    "- https://stackoverflow.com/questions/29545704/fast-haversine-approximation-python-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a22004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate distances between two points\n",
    "import numpy as np\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.    \n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bef9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to add distance in dataframe\n",
    "def add_trip_distance(df):\n",
    "    df['trip_distance'] = haversine_np(df['pickup_longitude'], \n",
    "                                          df['pickup_latitude'], \n",
    "                                          df['dropoff_longitude'], \n",
    "                                          df['dropoff_latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add trip distance in train set\n",
    "add_trip_distance(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbfdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add trip distance in validation set\n",
    "add_trip_distance(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f8165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add trip distance in test set\n",
    "add_trip_distance(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a6c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show train set\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd3e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show validation set\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce1f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show test set\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c3eb1",
   "metadata": {},
   "source": [
    "### Add Distance From Popular Landmarks\n",
    "\n",
    "> TIP: Creative feature engineering (generally involving human insights or external data) is a lot more effective than excessive hyperparameter tuning. Just one or two good feature improve the model's performance drastically.\n",
    "\n",
    "* JFK Airport\n",
    "* LGA Airport\n",
    "* EWR Airport\n",
    "* Times Square\n",
    "* Met Meuseum\n",
    "* World Trade Center\n",
    "\n",
    "Add distance from drop location. ( **Use distance from pickup location**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc0119",
   "metadata": {},
   "outputs": [],
   "source": [
    "jfk_lonlat = -73.7781, 40.6413\n",
    "lga_lonlat = -73.8740, 40.7769\n",
    "ewr_lonlat = -74.1745, 40.6895\n",
    "met_lonlat = -73.9632, 40.7794\n",
    "wtc_lonlat = -74.0099, 40.7126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate drop location distance from popular landmarks\n",
    "def add_landmark_dropoff_distance(df, landmark_name, landmark_lonlat):\n",
    "    lon, lat = landmark_lonlat\n",
    "    df[landmark_name + '_drop_distance'] = haversine_np(lon, lat, df['dropoff_longitude'], df['dropoff_latitude'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa14447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add distance data in dataframe\n",
    "def add_landmarks(a_df):\n",
    "    landmarks = [('jfk', jfk_lonlat), ('lga', lga_lonlat), ('ewr', ewr_lonlat), ('met', met_lonlat), ('wtc', wtc_lonlat)]\n",
    "    for name, lonlat in landmarks:\n",
    "        add_landmark_dropoff_distance(a_df, name, lonlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop distance from landmark in train set\n",
    "add_landmarks(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop distance from landmark in validation set\n",
    "add_landmarks(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ed50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop distance from landmark in test set\n",
    "add_landmarks(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show train set\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show validation set\n",
    "val_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show test set\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570a80a",
   "metadata": {},
   "source": [
    "### Remove Outliers and Invalid Data\n",
    "\n",
    "There seems to be some invalide data in each of the following columns:\n",
    "\n",
    "* Fare amount\n",
    "* Passenger count\n",
    "* Pickup latitude & longitude\n",
    "* Drop latitude & longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show summary statistics for numerical columns in train set\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a94a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show summary statistics for numerical columns in test set\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1749ebf",
   "metadata": {},
   "source": [
    "Use the following ranges from test data to filter the train and validation data:\n",
    "- `fare_amount`: \\$1 to \\$500\n",
    "- `longitudes`: -75 to -72\n",
    "- `latitudes`: 40 to 42\n",
    "- `passenger_count`: 1 to 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f0626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove outliers\n",
    "def remove_outliers(df):\n",
    "    return df[(df['fare_amount'] >= 1.) & \n",
    "              (df['fare_amount'] <= 500.) &\n",
    "              (df['pickup_longitude'] >= -75) & \n",
    "              (df['pickup_longitude'] <= -72) & \n",
    "              (df['dropoff_longitude'] >= -75) & \n",
    "              (df['dropoff_longitude'] <= -72) & \n",
    "              (df['pickup_latitude'] >= 40) & \n",
    "              (df['pickup_latitude'] <= 42) & \n",
    "              (df['dropoff_latitude'] >=40) & \n",
    "              (df['dropoff_latitude'] <= 42) & \n",
    "              (df['passenger_count'] >= 1) & \n",
    "              (df['passenger_count'] <= 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outlier from train set\n",
    "train_df = remove_outliers(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outlier from validation set\n",
    "val_df = remove_outliers(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a85a1",
   "metadata": {},
   "source": [
    "#### Scaling and One-Hot Encoding\n",
    "\n",
    "**Exercise**: Try scaling numeric columns to the `(0,1)` range and encoding categorical columns using a one-hot encoder.\n",
    "\n",
    "We won't do this because we'll be training tree-based models which are generally able to do a good job even without the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654175cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c69ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda08a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f7aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab81ccd7",
   "metadata": {},
   "source": [
    "### Save Intermediate DataFrames\n",
    "\n",
    "Let's save the processed datasets in the Apache Parquet format, so that we can load them back easily to resume our work from this point.\n",
    "\n",
    "You may also want to create differnt notebooks for EDA, feature engineering and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc22adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train data to compressed parquet format\n",
    "train_df.to_parquet('train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00398c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save validation data to compressed parquet format\n",
    "val_df.to_parquet('val.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test data to compressed parquet format\n",
    "test_df.to_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda73f0",
   "metadata": {},
   "source": [
    "## 7. Train & Evaluate Different Models\n",
    "\n",
    "Train each of the following & submit predictions to Kaggle:\n",
    "\n",
    "- Ridge Regression\n",
    "- Random Forests\n",
    "- Gradient Boosting\n",
    "\n",
    "Exercise: Train Ridge, SVM, KNN, Decision Tree models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209b074",
   "metadata": {},
   "source": [
    "### Split Inputs & Targes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dfca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of columns\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033da85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input columns\n",
    "input_cols = ['pickup_longitude', 'pickup_latitude',\n",
    "       'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "       'pickup_datetime_year', 'pickup_datetime_month', 'pickup_datetime_day',\n",
    "       'pickup_datetime_weekday', 'pickup_datetime_hour', 'trip_distance',\n",
    "       'jfk_drop_distance', 'lga_drop_distance', 'ewr_drop_distance',\n",
    "       'met_drop_distance', 'wtc_drop_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define target column\n",
    "target_col = 'fare_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d874d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train inputs and targets data\n",
    "train_inputs = train_df[input_cols]\n",
    "train_targets = train_df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation inputs and targets data\n",
    "val_inputs = val_df[input_cols]\n",
    "val_targets = val_df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eaadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test inputs data\n",
    "test_inputs = test_df[input_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b3933",
   "metadata": {},
   "source": [
    "Define a helper function to evaluate models and generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to evaluate models\n",
    "def evaluate(model):\n",
    "    train_preds = model.predict(train_inputs)\n",
    "    train_rmse = mean_squared_error(train_targets, train_preds, squared=False)\n",
    "    val_preds = model.predict(val_inputs)\n",
    "    val_rmse = mean_squared_error(val_targets, val_preds, squared=False)\n",
    "    return train_rmse, val_rmse, train_preds, val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f49451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to generate predictions\n",
    "def predict_and_submit(model, fname):\n",
    "    test_preds = model.predict(test_inputs)\n",
    "    sub_df = pd.read_csv('sample_submission.csv')\n",
    "    sub_df['fare_amount'] = test_preds\n",
    "    sub_df.to_csv(fname, index=None)\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1bd103",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b52621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ridge regression package from sklearn\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129837ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model1 = Ridge(random_state=42, alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a5f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with training set\n",
    "%time\n",
    "model1.fit(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b053a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set and generate submission file\n",
    "predict_and_submit(model1, 'ridge_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dfd085",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import random forest regression package from sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb77c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model2 = RandomForestRegressor(max_depth=10, n_jobs=-1, random_state=42, n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caaedc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with training set\n",
    "%time\n",
    "model2.fit(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8611e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "evaluate(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532bdcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set and generate submission file\n",
    "predict_and_submit(model2, 'rf_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da054ca3",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a8924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import gradient boosting package from sklearn\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model3 = XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdbca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with training set\n",
    "%time\n",
    "model3.fit(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410bb4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "evaluate(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set and generate submission file\n",
    "predict_and_submit(model3, 'xgb_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6242c963",
   "metadata": {},
   "source": [
    "## 8. Tune Hyperparmeters\n",
    "\n",
    "https://towardsdatascience.com/mastering-xgboost-2eb6bce6bc76\n",
    "\n",
    "\n",
    "We'll train parameters for the XGBoost model. Hereâ€™s a strategy for tuning hyperparameters:\n",
    "\n",
    "- Tune the most important/impactful hyperparameter first e.g. n_estimators\n",
    "\n",
    "- With the best value of the first hyperparameter, tune the next most impactful hyperparameter\n",
    "\n",
    "- And so on, keep training the next most impactful parameters with the best values for previous parameters...\n",
    "\n",
    "- Then, go back to the top and further tune each parameter again for further marginal gains\n",
    "\n",
    "- Hyperparameter tuning is more art than science, unfortunately. Check how the parameters interact with each other.\n",
    "\n",
    "Let's define a helper function for trying different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c136aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib package\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# function calculate training and validation error (root mean squared error)\n",
    "def test_params(ModelClass, **params):\n",
    "    \"\"\"Trains a model with the given parameters and returns training & validation RMSE\"\"\"\n",
    "    model = ModelClass(**params).fit(train_inputs, train_targets)\n",
    "    train_rmse = mean_squared_error(model.predict(train_inputs), train_targets, squared=False)\n",
    "    val_rmse = mean_squared_error(model.predict(val_inputs), val_targets, squared=False)\n",
    "    return train_rmse, val_rmse\n",
    "\n",
    "# function to plot training and validation error for each parameter\n",
    "def test_param_and_plot(ModelClass, param_name, param_values, **other_params):\n",
    "    \"\"\"Trains multiple models by varying the value of param_name according to param_values\"\"\"\n",
    "    train_errors, val_errors = [], [] \n",
    "    for value in param_values:\n",
    "        params = dict(other_params)\n",
    "        params[param_name] = value\n",
    "        train_rmse, val_rmse = test_params(ModelClass, **params)\n",
    "        train_errors.append(train_rmse)\n",
    "        val_errors.append(val_rmse)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title('Overfitting curve: ' + param_name)\n",
    "    plt.plot(param_values, train_errors, 'b-o')\n",
    "    plt.plot(param_values, val_errors, 'r-o')\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set best parameter\n",
    "best_params = {\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.05\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc025b5",
   "metadata": {},
   "source": [
    "### No. of Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using different n_estimators and plot the training and validation error\n",
    "%time \n",
    "test_param_and_plot(XGBRegressor, 'n_estimators', [100, 250, 500], **best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a013dd",
   "metadata": {},
   "source": [
    "Seems like 500 estimators has the lowest validation loss. However, it also takes a long time. Let's take 250 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d803ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set n_estimators to best value\n",
    "best_params['n_estimators'] = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bee2ea",
   "metadata": {},
   "source": [
    "### Max Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f372be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using different max_depth and plot the training and validation error\n",
    "%time \n",
    "test_param_and_plot(XGBRegressor, 'max_depth', [3, 4, 5, 7], **best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba344e1e",
   "metadata": {},
   "source": [
    "Looks like a max depth of 5 is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de45ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max_depth to best value\n",
    "best_params['max_depth'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24559b",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using different learning rates and plot the training and validation error\n",
    "%time\n",
    "test_param_and_plot(XGBRegressor, 'learning_rate', [0.05, 0.1, 0.25], **best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c804e6e7",
   "metadata": {},
   "source": [
    "Seems like the best learning rate is 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b447472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set learning_rate to best value\n",
    "best_params['learning_rate'] = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2889e39a",
   "metadata": {},
   "source": [
    "### Other Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a085a",
   "metadata": {},
   "source": [
    "Similarly we can experiment with other parameters. \n",
    "\n",
    "Here's a set of parameters that works well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa181f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final xgboost model object\n",
    "xgb_model_final = XGBRegressor(objective='reg:squarederror', \n",
    "                               n_jobs=-1, \n",
    "                               random_state=42,\n",
    "                               n_estimators=500, \n",
    "                               max_depth=8, \n",
    "                               learning_rate=0.08, \n",
    "                               subsample=0.8, \n",
    "                               colsample_bytree=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabdadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on training set\n",
    "%time\n",
    "xgb_model_final.fit(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef1539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "evaluate(xgb_model_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set and generate submission file\n",
    "predict_and_submit(xgb_model_final, 'xgb_tuned_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b184408c",
   "metadata": {},
   "source": [
    "Acieved 460th position out of 1483 i.e. top 30%.\n",
    "\n",
    "- We are using just 1% of the training data\n",
    "- We are only using a single model (most top submissions use ensembles)\n",
    "- Our best model takes just 10 minutes to train (as oppposed to hours/days)\n",
    "- We haven't fully optimized the hyperparameters yet\n",
    "\n",
    "Let's save the weights of this model. Follow this guide: https://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ab074",
   "metadata": {},
   "source": [
    "**Tasks**: \n",
    "\n",
    "1. Tune hyperparameters for Linear Regression & random forests.\n",
    "2. Repeat with 3%, 10%, 30% and 100% of the training set. How much reduction in error does 100x more data produce?\n",
    "3. Ensemble (average) the results from multiple models and observe if they're better than individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcefcd2",
   "metadata": {},
   "source": [
    "### Save Outputs to Google Drive (Optional)\n",
    "\n",
    "We can save all the output files we've created to Google Drive, so that we can reuse them later if required.\n",
    "\n",
    "Follow this guide: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5bdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d7d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de51f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcb511af",
   "metadata": {},
   "source": [
    "## 9. Train on GPU with entire dataset (Optional)\n",
    "\n",
    "Steps:\n",
    "- Install `dask`, `cudf` and `cuml`\n",
    "- Load the dataset to GPU\n",
    "- Create training and validation set\n",
    "- Perform feature engineering\n",
    "- Train XGBoost `cuml` model\n",
    "- Make predictions & submit\n",
    "\n",
    "Follow these guides and fill out the empty cells below:\n",
    "- https://towardsdatascience.com/nyc-taxi-fare-prediction-605159aa9c24\n",
    "- https://jovian.ai/allenkong221/nyc-taxi-fare-rapids-dask-gpu/v/1?utm_source=embed#C10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5092af29",
   "metadata": {},
   "source": [
    "### Install `dask`, `cudf` and `cuml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de5a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d5c9528",
   "metadata": {
    "id": "-T0mffaGIgTg"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15582d01",
   "metadata": {
    "id": "wbcghTdkIhaF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88053ab7",
   "metadata": {
    "id": "qbAgCLouIh0q"
   },
   "source": [
    "### Create training & validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ddfde2",
   "metadata": {
    "id": "e_2j3bfdIjjE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96ee9ac9",
   "metadata": {
    "id": "HD2FJpDRIkJo"
   },
   "source": [
    "### Perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f10bba4",
   "metadata": {
    "id": "OU7eGJ79Ilrs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d86e1e65",
   "metadata": {
    "id": "S5Fauiz4JNQa"
   },
   "source": [
    "### Train XGBoost model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4987e07",
   "metadata": {
    "id": "JolgKXPYJO3c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a644328",
   "metadata": {
    "id": "ZUFO3Id3JPbk"
   },
   "source": [
    "### Make Predictions & Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1783b0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49f402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78796028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765d01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1f5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da46c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70983f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53743a96",
   "metadata": {},
   "source": [
    "## 10. Document & Publish Your Work\n",
    "\n",
    "- Add explanations using Markdown\n",
    "- Clean up the code & create functions\n",
    "- Publish notebook to Jovian\n",
    "- Write a blog post and embed\n",
    "\n",
    "Follow this guide: https://www.youtube.com/watch?v=NK6UYg3-Bxs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d68e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## References\n",
    "\n",
    "* Dataset: https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview\n",
    "* Missing semester (Shell scripting): https://missing.csail.mit.edu/\n",
    "* Opendatsets library: https://github.com/JovianML/opendatasets \n",
    "* EDA project from scratch: https://www.youtube.com/watch?v=kLDTbavcmd0\n",
    "* GeoPy: https://geopy.readthedocs.io/en/stable/#module-geopy.distance \n",
    "* Blog post by Allen Kong: https://towardsdatascience.com/nyc-taxi-fare-prediction-605159aa9c24 \n",
    "* Machine Learning with Python: Zero to GBMs - https://zerotogbms.com \n",
    "* Experiment tracking spreadsheet: https://bit.ly/mltrackingsheet \n",
    "* Pandas datetime components: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-date-components \n",
    "* Haversine distance: https://en.wikipedia.org/wiki/Haversine_formula \n",
    "* Haversine distance with Numpy: https://stackoverflow.com/questions/29545704/fast-haversine-approximation-python-pandas \n",
    "* RAPIDS (parent project for cudf and cuml): https://rapids.ai/\n",
    "* Data Science blog post from scratch: https://www.youtube.com/watch?v=NK6UYg3-Bxs \n",
    "* Examples of Machine Learning Projects:\n",
    "    * Walmart Store Sales: https://jovian.ai/anushree-k/final-walmart-simple-rf-gbm\n",
    "    * Used Car Price Prediction: https://jovian.ai/kara-mounir/used-cars-prices \n",
    "    * Lithology Prediction: https://jovian.ai/ramysaleem/ml-project-machine-predicting-lithologies\n",
    "    * Ad Demand Prediction: https://jovian.ai/deepa-sarojam/online-ad-demand-prediction-ml-prj \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01942e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b9786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
